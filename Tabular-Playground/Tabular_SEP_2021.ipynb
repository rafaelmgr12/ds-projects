{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tabular-SEP-2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fQ9qz0NJIxXN",
        "Fupz45o118Cn",
        "QdeE6hWgKBg4",
        "GuDBeAQFL1e2"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNkBA9jLjsv6pUb/pYjmcFC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaelmgr12/ds-projects/blob/main/Tabular-Playground/Tabular_SEP_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TZ_OUSeIsbH"
      },
      "source": [
        "# Tabular Playground Series - Sep 2021\n",
        "Practice your ML skills on this approachable dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ9qz0NJIxXN"
      },
      "source": [
        "## Setup  Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHTms5YEzRq7"
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "  from google.colab import files\n",
        "  files.upload()\n",
        "  ! mkdir ~/.kaggle\n",
        "  ! wget https://raw.githubusercontent.com/rafaelmgr12/ds-projects/main/functions/ml_functions.py -O ml_functions.py\n",
        "  ! wget https://raw.githubusercontent.com/rafaelmgr12/ds-projects/main/functions/plots_functions.py -O plot_functions.py\n",
        "  ! pip install sentencepiece\n",
        "  ! pip install optuna\n",
        "  ! pip install unidecode\n",
        "  ! pip install catboost\n",
        "  ! pip install category-encoders\n",
        "  ! cp kaggle.json ~/.kaggle/\n",
        "  ! chmod 600 ~/.kaggle/kaggle.json\n",
        "  ! kaggle competitions download -c tabular-playground-series-sep-2021\n",
        "  ! unzip test.csv.zip && unzip train.csv.zip && unzip sample_solution.csv.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hujUbpauK-pV"
      },
      "source": [
        "## Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOxIc8UzZhTI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "# data preparation\n",
        "from sklearn.model_selection import KFold,train_test_split,RepeatedKFold,StratifiedKFold,train_test_split\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "import sklearn.metrics                                                                              \n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "# ml agorthims\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "import catboost as cb\n",
        "from catboost import datasets, CatBoostClassifier\n",
        "from catboost import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYxpo-9VLDoc"
      },
      "source": [
        "Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CGdkXRUzpGi"
      },
      "source": [
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "Xq86I7KT1Wi1",
        "outputId": "8c2f476e-341c-4650-b2da-db1049bfd0d5"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>f11</th>\n",
              "      <th>f12</th>\n",
              "      <th>f13</th>\n",
              "      <th>f14</th>\n",
              "      <th>f15</th>\n",
              "      <th>f16</th>\n",
              "      <th>f17</th>\n",
              "      <th>f18</th>\n",
              "      <th>f19</th>\n",
              "      <th>f20</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f23</th>\n",
              "      <th>f24</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f27</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>f31</th>\n",
              "      <th>f32</th>\n",
              "      <th>f33</th>\n",
              "      <th>f34</th>\n",
              "      <th>f35</th>\n",
              "      <th>f36</th>\n",
              "      <th>f37</th>\n",
              "      <th>f38</th>\n",
              "      <th>f39</th>\n",
              "      <th>...</th>\n",
              "      <th>f80</th>\n",
              "      <th>f81</th>\n",
              "      <th>f82</th>\n",
              "      <th>f83</th>\n",
              "      <th>f84</th>\n",
              "      <th>f85</th>\n",
              "      <th>f86</th>\n",
              "      <th>f87</th>\n",
              "      <th>f88</th>\n",
              "      <th>f89</th>\n",
              "      <th>f90</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>f100</th>\n",
              "      <th>f101</th>\n",
              "      <th>f102</th>\n",
              "      <th>f103</th>\n",
              "      <th>f104</th>\n",
              "      <th>f105</th>\n",
              "      <th>f106</th>\n",
              "      <th>f107</th>\n",
              "      <th>f108</th>\n",
              "      <th>f109</th>\n",
              "      <th>f110</th>\n",
              "      <th>f111</th>\n",
              "      <th>f112</th>\n",
              "      <th>f113</th>\n",
              "      <th>f114</th>\n",
              "      <th>f115</th>\n",
              "      <th>f116</th>\n",
              "      <th>f117</th>\n",
              "      <th>f118</th>\n",
              "      <th>claim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.10859</td>\n",
              "      <td>0.004314</td>\n",
              "      <td>-37.566</td>\n",
              "      <td>0.017364</td>\n",
              "      <td>0.28915</td>\n",
              "      <td>-10.25100</td>\n",
              "      <td>135.12</td>\n",
              "      <td>168900.0</td>\n",
              "      <td>3.992400e+14</td>\n",
              "      <td>86.489</td>\n",
              "      <td>0.59881</td>\n",
              "      <td>1.423200e+09</td>\n",
              "      <td>0.27240</td>\n",
              "      <td>9.455600</td>\n",
              "      <td>-0.050305</td>\n",
              "      <td>1938.300</td>\n",
              "      <td>8.6331</td>\n",
              "      <td>4.0607</td>\n",
              "      <td>26.8670</td>\n",
              "      <td>-1.180</td>\n",
              "      <td>10961.00</td>\n",
              "      <td>1.5397</td>\n",
              "      <td>135.3200</td>\n",
              "      <td>-1.49650</td>\n",
              "      <td>440.080</td>\n",
              "      <td>2.590100e+12</td>\n",
              "      <td>2.194200e+09</td>\n",
              "      <td>2968800.0</td>\n",
              "      <td>0.001431</td>\n",
              "      <td>13.3270</td>\n",
              "      <td>0.75050</td>\n",
              "      <td>18509.0</td>\n",
              "      <td>146820.0</td>\n",
              "      <td>-0.000276</td>\n",
              "      <td>1.090600e+16</td>\n",
              "      <td>1705.400</td>\n",
              "      <td>414.29</td>\n",
              "      <td>3.5392</td>\n",
              "      <td>1888.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001081</td>\n",
              "      <td>6.1244</td>\n",
              "      <td>1.231800e+11</td>\n",
              "      <td>275.9200</td>\n",
              "      <td>5308500.0</td>\n",
              "      <td>1704.000</td>\n",
              "      <td>5.022400e+10</td>\n",
              "      <td>53.3980</td>\n",
              "      <td>-2.2012</td>\n",
              "      <td>6871.0</td>\n",
              "      <td>3.8862</td>\n",
              "      <td>-0.00558</td>\n",
              "      <td>5252.100</td>\n",
              "      <td>166.690</td>\n",
              "      <td>1.60740</td>\n",
              "      <td>0.66534</td>\n",
              "      <td>7768.900</td>\n",
              "      <td>0.99662</td>\n",
              "      <td>1.125700e+11</td>\n",
              "      <td>2.2432</td>\n",
              "      <td>0.934160</td>\n",
              "      <td>0.65056</td>\n",
              "      <td>94569.0</td>\n",
              "      <td>21.471</td>\n",
              "      <td>8214.100</td>\n",
              "      <td>0.288010</td>\n",
              "      <td>0.097826</td>\n",
              "      <td>0.001071</td>\n",
              "      <td>1.412400e+09</td>\n",
              "      <td>0.11093</td>\n",
              "      <td>-12.2280</td>\n",
              "      <td>1.7482</td>\n",
              "      <td>1.90960</td>\n",
              "      <td>-7.11570</td>\n",
              "      <td>4378.80</td>\n",
              "      <td>1.2096</td>\n",
              "      <td>8.613400e+14</td>\n",
              "      <td>140.1</td>\n",
              "      <td>1.01770</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.10090</td>\n",
              "      <td>0.299610</td>\n",
              "      <td>11822.000</td>\n",
              "      <td>0.276500</td>\n",
              "      <td>0.45970</td>\n",
              "      <td>-0.83733</td>\n",
              "      <td>1721.90</td>\n",
              "      <td>119810.0</td>\n",
              "      <td>3.874100e+15</td>\n",
              "      <td>9953.600</td>\n",
              "      <td>1.20930</td>\n",
              "      <td>3.334100e+09</td>\n",
              "      <td>0.28631</td>\n",
              "      <td>-0.012858</td>\n",
              "      <td>-0.019912</td>\n",
              "      <td>10.284</td>\n",
              "      <td>6.1872</td>\n",
              "      <td>1.0419</td>\n",
              "      <td>4.6404</td>\n",
              "      <td>31.877</td>\n",
              "      <td>123620.00</td>\n",
              "      <td>1.3951</td>\n",
              "      <td>125.8100</td>\n",
              "      <td>1.19890</td>\n",
              "      <td>136.450</td>\n",
              "      <td>9.098100e+09</td>\n",
              "      <td>4.004100e+10</td>\n",
              "      <td>1564000.0</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>3.1074</td>\n",
              "      <td>1.50330</td>\n",
              "      <td>238000.0</td>\n",
              "      <td>21440.0</td>\n",
              "      <td>-0.001344</td>\n",
              "      <td>3.079400e+16</td>\n",
              "      <td>229.100</td>\n",
              "      <td>844.82</td>\n",
              "      <td>1.4680</td>\n",
              "      <td>4726.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.254100</td>\n",
              "      <td>6.9191</td>\n",
              "      <td>1.832400e+11</td>\n",
              "      <td>9.6510</td>\n",
              "      <td>32800.0</td>\n",
              "      <td>1480.600</td>\n",
              "      <td>2.300600e+10</td>\n",
              "      <td>44.0510</td>\n",
              "      <td>205.6900</td>\n",
              "      <td>4295.3</td>\n",
              "      <td>13.3880</td>\n",
              "      <td>0.46843</td>\n",
              "      <td>754.610</td>\n",
              "      <td>83.233</td>\n",
              "      <td>1.18900</td>\n",
              "      <td>29.55000</td>\n",
              "      <td>7343.700</td>\n",
              "      <td>0.99815</td>\n",
              "      <td>4.877700e+13</td>\n",
              "      <td>1.2708</td>\n",
              "      <td>-0.000969</td>\n",
              "      <td>5.29520</td>\n",
              "      <td>6779.0</td>\n",
              "      <td>227.720</td>\n",
              "      <td>34.342</td>\n",
              "      <td>0.340300</td>\n",
              "      <td>0.143370</td>\n",
              "      <td>0.049276</td>\n",
              "      <td>1.903200e+09</td>\n",
              "      <td>0.97673</td>\n",
              "      <td>-56.7580</td>\n",
              "      <td>4.1684</td>\n",
              "      <td>0.34808</td>\n",
              "      <td>4.14200</td>\n",
              "      <td>913.23</td>\n",
              "      <td>1.2464</td>\n",
              "      <td>7.575100e+15</td>\n",
              "      <td>1861.0</td>\n",
              "      <td>0.28359</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.17803</td>\n",
              "      <td>-0.006980</td>\n",
              "      <td>907.270</td>\n",
              "      <td>0.272140</td>\n",
              "      <td>0.45948</td>\n",
              "      <td>0.17327</td>\n",
              "      <td>2298.00</td>\n",
              "      <td>360650.0</td>\n",
              "      <td>1.224500e+13</td>\n",
              "      <td>15827.000</td>\n",
              "      <td>0.38164</td>\n",
              "      <td>1.230300e+09</td>\n",
              "      <td>0.25807</td>\n",
              "      <td>2.455600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26.873</td>\n",
              "      <td>7.5463</td>\n",
              "      <td>1.9967</td>\n",
              "      <td>1.9526</td>\n",
              "      <td>817.760</td>\n",
              "      <td>-2948.70</td>\n",
              "      <td>2.0054</td>\n",
              "      <td>1.6826</td>\n",
              "      <td>1.19680</td>\n",
              "      <td>74.624</td>\n",
              "      <td>-3.273900e+10</td>\n",
              "      <td>5.718900e+10</td>\n",
              "      <td>11058.0</td>\n",
              "      <td>-0.003097</td>\n",
              "      <td>8.0241</td>\n",
              "      <td>1.13180</td>\n",
              "      <td>27940.0</td>\n",
              "      <td>862460.0</td>\n",
              "      <td>-0.002207</td>\n",
              "      <td>5.849100e+13</td>\n",
              "      <td>-897.840</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.3561</td>\n",
              "      <td>3063.4</td>\n",
              "      <td>...</td>\n",
              "      <td>0.260260</td>\n",
              "      <td>6.1052</td>\n",
              "      <td>1.013300e+11</td>\n",
              "      <td>357.2700</td>\n",
              "      <td>1476600.0</td>\n",
              "      <td>90.845</td>\n",
              "      <td>1.306200e+09</td>\n",
              "      <td>2.3731</td>\n",
              "      <td>391.3700</td>\n",
              "      <td>2965.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49459</td>\n",
              "      <td>43.524</td>\n",
              "      <td>138.520</td>\n",
              "      <td>1.10790</td>\n",
              "      <td>0.91948</td>\n",
              "      <td>47.915</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.510500e+12</td>\n",
              "      <td>3.4663</td>\n",
              "      <td>0.560950</td>\n",
              "      <td>4.13090</td>\n",
              "      <td>95531.0</td>\n",
              "      <td>39.486</td>\n",
              "      <td>-83.148</td>\n",
              "      <td>0.084881</td>\n",
              "      <td>0.032222</td>\n",
              "      <td>0.001668</td>\n",
              "      <td>1.436500e+07</td>\n",
              "      <td>0.20102</td>\n",
              "      <td>-5.7688</td>\n",
              "      <td>1.2042</td>\n",
              "      <td>0.26290</td>\n",
              "      <td>8.13120</td>\n",
              "      <td>45119.00</td>\n",
              "      <td>1.1764</td>\n",
              "      <td>3.218100e+14</td>\n",
              "      <td>3838.2</td>\n",
              "      <td>0.40690</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.15236</td>\n",
              "      <td>0.007259</td>\n",
              "      <td>780.100</td>\n",
              "      <td>0.025179</td>\n",
              "      <td>0.51947</td>\n",
              "      <td>7.49140</td>\n",
              "      <td>112.51</td>\n",
              "      <td>259490.0</td>\n",
              "      <td>7.781400e+13</td>\n",
              "      <td>-36.837</td>\n",
              "      <td>1.10960</td>\n",
              "      <td>1.223100e+09</td>\n",
              "      <td>0.30944</td>\n",
              "      <td>10.370000</td>\n",
              "      <td>-0.106260</td>\n",
              "      <td>533.840</td>\n",
              "      <td>7.8490</td>\n",
              "      <td>1.0379</td>\n",
              "      <td>8.0030</td>\n",
              "      <td>12.349</td>\n",
              "      <td>-195.28</td>\n",
              "      <td>2.5598</td>\n",
              "      <td>92.1420</td>\n",
              "      <td>0.63789</td>\n",
              "      <td>1054.900</td>\n",
              "      <td>-1.204100e+10</td>\n",
              "      <td>5.187300e+12</td>\n",
              "      <td>1475400.0</td>\n",
              "      <td>1.036500</td>\n",
              "      <td>1.1903</td>\n",
              "      <td>0.98941</td>\n",
              "      <td>301200.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>-9.299200e+13</td>\n",
              "      <td>-10.818</td>\n",
              "      <td>1020.30</td>\n",
              "      <td>2.9553</td>\n",
              "      <td>3342.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.372830</td>\n",
              "      <td>1.5606</td>\n",
              "      <td>1.835400e+10</td>\n",
              "      <td>-3.4298</td>\n",
              "      <td>6485700.0</td>\n",
              "      <td>2120.000</td>\n",
              "      <td>3.081200e+10</td>\n",
              "      <td>34.0560</td>\n",
              "      <td>157.4300</td>\n",
              "      <td>3724.5</td>\n",
              "      <td>8.4211</td>\n",
              "      <td>0.40778</td>\n",
              "      <td>2971.200</td>\n",
              "      <td>204.700</td>\n",
              "      <td>-0.97998</td>\n",
              "      <td>9.94050</td>\n",
              "      <td>12011.000</td>\n",
              "      <td>0.99898</td>\n",
              "      <td>5.063400e+13</td>\n",
              "      <td>1.2261</td>\n",
              "      <td>0.250200</td>\n",
              "      <td>0.72974</td>\n",
              "      <td>373690.0</td>\n",
              "      <td>194.650</td>\n",
              "      <td>120.930</td>\n",
              "      <td>0.260710</td>\n",
              "      <td>0.234240</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>1.442300e+09</td>\n",
              "      <td>-0.01182</td>\n",
              "      <td>-34.8580</td>\n",
              "      <td>2.0694</td>\n",
              "      <td>0.79631</td>\n",
              "      <td>-16.33600</td>\n",
              "      <td>4952.40</td>\n",
              "      <td>1.1784</td>\n",
              "      <td>4.533000e+12</td>\n",
              "      <td>4889.1</td>\n",
              "      <td>0.51486</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.11623</td>\n",
              "      <td>0.502900</td>\n",
              "      <td>-109.150</td>\n",
              "      <td>0.297910</td>\n",
              "      <td>0.34490</td>\n",
              "      <td>-0.40932</td>\n",
              "      <td>2538.90</td>\n",
              "      <td>65332.0</td>\n",
              "      <td>1.907200e+15</td>\n",
              "      <td>144.120</td>\n",
              "      <td>1.05310</td>\n",
              "      <td>2.634100e+09</td>\n",
              "      <td>0.29782</td>\n",
              "      <td>2.654800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1808.900</td>\n",
              "      <td>7.2783</td>\n",
              "      <td>3.9757</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29520.00</td>\n",
              "      <td>3.4225</td>\n",
              "      <td>96.7250</td>\n",
              "      <td>0.79725</td>\n",
              "      <td>215.570</td>\n",
              "      <td>1.732600e+13</td>\n",
              "      <td>2.635200e+12</td>\n",
              "      <td>2161200.0</td>\n",
              "      <td>0.895470</td>\n",
              "      <td>6.8257</td>\n",
              "      <td>0.97413</td>\n",
              "      <td>142620.0</td>\n",
              "      <td>231350.0</td>\n",
              "      <td>0.001257</td>\n",
              "      <td>1.012500e+16</td>\n",
              "      <td>51.508</td>\n",
              "      <td>293.76</td>\n",
              "      <td>1.3351</td>\n",
              "      <td>3042.1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.085690</td>\n",
              "      <td>1.5846</td>\n",
              "      <td>3.825200e+10</td>\n",
              "      <td>130.7000</td>\n",
              "      <td>102100.0</td>\n",
              "      <td>1951.800</td>\n",
              "      <td>1.142800e+10</td>\n",
              "      <td>58.5660</td>\n",
              "      <td>176.8300</td>\n",
              "      <td>1279.0</td>\n",
              "      <td>4.9662</td>\n",
              "      <td>0.47912</td>\n",
              "      <td>-70.278</td>\n",
              "      <td>10.887</td>\n",
              "      <td>1.14340</td>\n",
              "      <td>6.19120</td>\n",
              "      <td>197.470</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.574800e+13</td>\n",
              "      <td>1.0083</td>\n",
              "      <td>0.339530</td>\n",
              "      <td>13.48600</td>\n",
              "      <td>201300.0</td>\n",
              "      <td>38.842</td>\n",
              "      <td>324.000</td>\n",
              "      <td>0.238250</td>\n",
              "      <td>0.141550</td>\n",
              "      <td>0.002208</td>\n",
              "      <td>5.830700e+09</td>\n",
              "      <td>0.92739</td>\n",
              "      <td>-13.6410</td>\n",
              "      <td>1.5298</td>\n",
              "      <td>1.14640</td>\n",
              "      <td>-0.43124</td>\n",
              "      <td>3856.50</td>\n",
              "      <td>1.4830</td>\n",
              "      <td>-8.991300e+12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.23049</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 120 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id       f1        f2         f3  ...          f116    f117     f118  claim\n",
              "0   0  0.10859  0.004314    -37.566  ...  8.613400e+14   140.1  1.01770      1\n",
              "1   1  0.10090  0.299610  11822.000  ...  7.575100e+15  1861.0  0.28359      0\n",
              "2   2  0.17803 -0.006980    907.270  ...  3.218100e+14  3838.2  0.40690      1\n",
              "3   3  0.15236  0.007259    780.100  ...  4.533000e+12  4889.1  0.51486      1\n",
              "4   4  0.11623  0.502900   -109.150  ... -8.991300e+12     NaN  0.23049      1\n",
              "\n",
              "[5 rows x 120 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq5ZrruvLFn_"
      },
      "source": [
        "Check for Imbalanced data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "R393Jh33f8sR",
        "outputId": "92d2450f-8259-4aaf-96c8-153e342eb337"
      },
      "source": [
        "sns.countplot(train_df['claim'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning:\n",
            "\n",
            "Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe937b48a90>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD_nnQr3LL5I"
      },
      "source": [
        "We have null Values in our dataset, therefore a inputer method with median was chosen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8H9K0F82j-u"
      },
      "source": [
        "imp =  SimpleImputer(missing_values=np.nan, strategy='median')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxYI0lb92KOs"
      },
      "source": [
        "X = train_df.drop(['id','claim'],axis = 1)\n",
        "y = train_df.claim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUkNv9x-RwLs"
      },
      "source": [
        "X = imp.fit_transform(X)\n",
        "X_test = imp.fit_transform(test_df.drop('id',axis = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgV5i7P5S3LV"
      },
      "source": [
        "rob = RobustScaler()\n",
        "X = rob.fit_transform(X)\n",
        "X_test = rob.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C3hh-0eLaq-"
      },
      "source": [
        "## Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fupz45o118Cn"
      },
      "source": [
        "### LightGMB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUB0X2kpk-D9"
      },
      "source": [
        "tunning = False # True, if you want to hypertunning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df9mC5ztj20f"
      },
      "source": [
        "if tunning :\n",
        " \n",
        "  def objective(trial):\n",
        "      train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25)\n",
        "      dtrain = lgb.Dataset(train_x, label=train_y)\n",
        "  \n",
        "      param = {\n",
        "          'objective': 'binary',\n",
        "          'metric': 'auc',\n",
        "          'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
        "          'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
        "          'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
        "          'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
        "          'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
        "          'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
        "          'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "      }\n",
        "  \n",
        "      gbm = lgb.train(param, dtrain)\n",
        "      preds = gbm.predict(test_x)\n",
        "      pred_labels = np.rint(preds)\n",
        "      accuracy = accuracy_score(test_y, pred_labels)\n",
        "      return accuracy\n",
        "  \n",
        "  study = optuna.create_study(direction='maximize')\n",
        "  study.optimize(objective, n_trials=50)\n",
        "  \n",
        "  print('Number of finished trials:', len(study.trials))\n",
        "  print('Best trial:', study.best_trial.params)\n",
        "  param = study.best_trial.params\n",
        "  param\n",
        "  param['objective'] = 'binary'\n",
        "else :\n",
        "  param = {'bagging_fraction': 0.9556136342570964,\n",
        " 'bagging_freq': 7,\n",
        " 'feature_fraction': 0.9093664719465578,\n",
        " 'lambda_l1': 0.0009731112192839761,\n",
        " 'lambda_l2': 0.05022674259693464,\n",
        " 'metric': 'auc',\n",
        " 'min_child_samples': 53,\n",
        " 'num_leaves': 136,\n",
        " 'objective': 'binary'}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h-KKRwMheBL"
      },
      "source": [
        "features = test_df.drop('id',axis = 1).columns.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpcAwoIhg23-",
        "outputId": "4dfddf8f-22d2-45d5-ca04-2e17d8684900"
      },
      "source": [
        "folds = KFold(n_splits=10, shuffle=True, random_state=None)\n",
        "oof = np.zeros(len(train_df))\n",
        "predictions_lgbm = np.zeros(len(test_df))\n",
        "feature_importance_df = pd.DataFrame()\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
        "    fold_ += 1\n",
        "    print(\"Fold {}\".format(fold_))\n",
        "    trn_data = lgb.Dataset(X[trn_idx], label=y[trn_idx])\n",
        "    val_data = lgb.Dataset(X[val_idx], label=y[val_idx])\n",
        "\n",
        "    num_round = 10000\n",
        "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 300)\n",
        "    oof[val_idx] = clf.predict(X[val_idx], num_iteration=clf.best_iteration)\n",
        "    \n",
        "    fold_importance_df = pd.DataFrame()\n",
        "    fold_importance_df[\"Feature\"] = features\n",
        "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
        "    fold_importance_df[\"fold\"] = fold_ + 1\n",
        "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "    \n",
        "    predictions_lgbm += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1\n",
            "Training until validation scores don't improve for 300 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[372]\ttraining's auc: 0.860246\tvalid_1's auc: 0.788205\n",
            "Fold 2\n",
            "Training until validation scores don't improve for 300 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[433]\ttraining's auc: 0.86888\tvalid_1's auc: 0.79137\n",
            "Fold 3\n",
            "Training until validation scores don't improve for 300 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[312]\ttraining's auc: 0.851007\tvalid_1's auc: 0.788776\n",
            "Fold 4\n",
            "Training until validation scores don't improve for 300 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[502]\ttraining's auc: 0.878224\tvalid_1's auc: 0.790514\n",
            "Fold 5\n",
            "Training until validation scores don't improve for 300 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[345]\ttraining's auc: 0.855895\tvalid_1's auc: 0.789938\n",
            "Fold 6\n",
            "Training until validation scores don't improve for 300 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[399]\ttraining's auc: 0.864155\tvalid_1's auc: 0.789552\n",
            "Fold 7\n",
            "Training until validation scores don't improve for 300 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[379]\ttraining's auc: 0.861104\tvalid_1's auc: 0.788705\n",
            "Fold 8\n",
            "Training until validation scores don't improve for 300 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[374]\ttraining's auc: 0.860307\tvalid_1's auc: 0.788178\n",
            "Fold 9\n",
            "Training until validation scores don't improve for 300 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[289]\ttraining's auc: 0.847092\tvalid_1's auc: 0.789711\n",
            "Fold 10\n",
            "Training until validation scores don't improve for 300 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[451]\ttraining's auc: 0.871562\tvalid_1's auc: 0.787764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpBnYERWri2N"
      },
      "source": [
        "preds_ac = [int(i > .5) for i in oof]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlXYUAjxehQ9",
        "outputId": "68dfc19e-b062-44fa-8d27-e99d67aea3c7"
      },
      "source": [
        "print(\"\\n\\n\")\n",
        "print('-'*1000)\n",
        "print(\"ROC CV score: {:<8.5f}\".format(roc_auc_score(train_df['claim'].values, oof)))\n",
        "print(\"Accuracy CV score: {:<8.5f}\".format(accuracy_score(train_df['claim'].values, preds_ac)))\n",
        "print('-'*1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ROC CV score: 0.78921 \n",
            "Accuracy CV score: 0.72588 \n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdeE6hWgKBg4"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcWif5EKBXy",
        "outputId": "40ffc399-4fb3-4e5c-f4e5-9be22b0a2459"
      },
      "source": [
        "tunning = False\n",
        "\n",
        "if tunning:\n",
        "\n",
        "  def objective(trial):\n",
        "      #(data, target) = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
        "      train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\n",
        "      dtrain = xgb.DMatrix(train_x, label=train_y)\n",
        "      dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
        "\n",
        "      param = {\n",
        "          \"verbosity\": 0,\n",
        "          \"objective\": \"binary:logistic\",\n",
        "          \"eval_metric\": 'auc',\n",
        "          # use exact for small dataset.\n",
        "          \"tree_method\": \"exact\",\n",
        "          # defines booster, gblinear for linear functions.\n",
        "          \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
        "          # L2 regularization weight.\n",
        "          \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
        "          # L1 regularization weight.\n",
        "          \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
        "          # sampling ratio for training data.\n",
        "          \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
        "          # sampling according to each tree.\n",
        "          \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
        "      }\n",
        "\n",
        "      if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
        "          # maximum depth of the tree, signifies complexity of the tree.\n",
        "          param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
        "          # minimum child weight, larger the term more conservative the tree.\n",
        "          param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
        "          param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
        "          # defines how selective algorithm is.\n",
        "          param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
        "          param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
        "\n",
        "      if param[\"booster\"] == \"dart\":\n",
        "          param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
        "          param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
        "          param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
        "          param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
        "\n",
        "      bst = xgb.train(param, dtrain)\n",
        "      preds = bst.predict(dvalid)\n",
        "      pred_labels = np.rint(preds)\n",
        "      accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n",
        "      return accuracy\n",
        "\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      study = optuna.create_study(direction=\"maximize\")\n",
        "      study.optimize(objective, n_trials=100, timeout=600)\n",
        "\n",
        "      print(\"Number of finished trials: \", len(study.trials))\n",
        "      print(\"Best trial:\")\n",
        "      trial = study.best_trial\n",
        "\n",
        "      print(\"  Value: {}\".format(trial.value))\n",
        "      print(\"  Params: \")\n",
        "      for key, value in trial.params.items():\n",
        "          print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "      param = study.best_trial.params\n",
        "      param['eval_metric'] = 'auc'\n",
        "else:\n",
        "  param = {'alpha': 0.9286263881717246,\n",
        "  'eval_metric' : 'auc',\n",
        " 'booster': 'gbtree',\n",
        " 'colsample_bytree': 0.7317911276911447,\n",
        " 'eta': 0.4994795364431274,\n",
        " 'gamma': 1.7554366442993805e-08,\n",
        " 'grow_policy': 'lossguide',\n",
        " 'lambda': 1.0681330380503332e-08,\n",
        " 'max_depth': 9,\n",
        " 'min_child_weight': 5,\n",
        " 'subsample': 0.9961101392862352}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-09-05 13:36:41,820]\u001b[0m A new study created in memory with name: no-name-51c91e8d-f96d-4273-8888-dbf8c2f6c6fe\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:37:28,440]\u001b[0m Trial 0 finished with value: 0.5328712209787874 and parameters: {'booster': 'dart', 'lambda': 3.9147528429830035e-08, 'alpha': 3.8770723344403604e-05, 'subsample': 0.8285429311623191, 'colsample_bytree': 0.7408497468110313, 'max_depth': 5, 'min_child_weight': 7, 'eta': 0.002113324322335961, 'gamma': 5.509691201816342e-05, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.23481257951118759, 'skip_drop': 9.857934748711291e-06}. Best is trial 0 with value: 0.5328712209787874.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:37:45,672]\u001b[0m Trial 1 finished with value: 0.5382286621012193 and parameters: {'booster': 'dart', 'lambda': 0.0001689990698384423, 'alpha': 2.6037387377976437e-08, 'subsample': 0.9671927789870047, 'colsample_bytree': 0.37940518266289774, 'max_depth': 3, 'min_child_weight': 8, 'eta': 0.0015558353770261974, 'gamma': 0.0052345582720113625, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.9922525300376315e-06, 'skip_drop': 1.1483275937824803e-08}. Best is trial 1 with value: 0.5382286621012193.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:37:55,083]\u001b[0m Trial 2 finished with value: 0.526636879906464 and parameters: {'booster': 'gblinear', 'lambda': 0.010539186039799454, 'alpha': 0.00012153728865141897, 'subsample': 0.9707868326805003, 'colsample_bytree': 0.5006725808065553}. Best is trial 1 with value: 0.5382286621012193.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:38:07,088]\u001b[0m Trial 3 finished with value: 0.5173375647235677 and parameters: {'booster': 'gblinear', 'lambda': 8.117579607983679e-07, 'alpha': 0.005026982887210642, 'subsample': 0.3427078426130022, 'colsample_bytree': 0.32303334981036386}. Best is trial 1 with value: 0.5382286621012193.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:38:16,918]\u001b[0m Trial 4 finished with value: 0.5271964255887757 and parameters: {'booster': 'gblinear', 'lambda': 1.1826290414508146e-06, 'alpha': 2.2842021967714742e-06, 'subsample': 0.6378284764542933, 'colsample_bytree': 0.4216275817714825}. Best is trial 1 with value: 0.5382286621012193.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:39:06,695]\u001b[0m Trial 5 finished with value: 0.5573701352931352 and parameters: {'booster': 'gbtree', 'lambda': 1.0472400991725427e-06, 'alpha': 0.015900702871701825, 'subsample': 0.22888582242860311, 'colsample_bytree': 0.7036880749437937, 'max_depth': 9, 'min_child_weight': 9, 'eta': 5.19810207507607e-06, 'gamma': 1.0067484750392814e-06, 'grow_policy': 'lossguide'}. Best is trial 5 with value: 0.5573701352931352.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:39:52,429]\u001b[0m Trial 6 finished with value: 0.5549189911474862 and parameters: {'booster': 'dart', 'lambda': 0.015828739129053384, 'alpha': 0.3778163947386615, 'subsample': 0.7084828609103653, 'colsample_bytree': 0.6320453973782099, 'max_depth': 7, 'min_child_weight': 7, 'eta': 9.772532296806703e-06, 'gamma': 2.2422992204363162e-06, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.2532867065715524, 'skip_drop': 0.00024374326786791216}. Best is trial 5 with value: 0.5573701352931352.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:40:00,568]\u001b[0m Trial 7 finished with value: 0.5011817270753299 and parameters: {'booster': 'gblinear', 'lambda': 1.1349996421861817e-08, 'alpha': 0.11753740705899453, 'subsample': 0.4352995320972496, 'colsample_bytree': 0.9422028937989426}. Best is trial 5 with value: 0.5573701352931352.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:40:54,098]\u001b[0m Trial 8 finished with value: 0.5750751628528479 and parameters: {'booster': 'gbtree', 'lambda': 2.560037555787364e-08, 'alpha': 0.0011152625651689317, 'subsample': 0.8631376411113916, 'colsample_bytree': 0.47763067226328926, 'max_depth': 9, 'min_child_weight': 2, 'eta': 1.4578403485539273e-05, 'gamma': 0.09540987563212086, 'grow_policy': 'lossguide'}. Best is trial 8 with value: 0.5750751628528479.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:41:03,005]\u001b[0m Trial 9 finished with value: 0.5299440454317689 and parameters: {'booster': 'gblinear', 'lambda': 3.47513199046926e-07, 'alpha': 1.119182014489494e-05, 'subsample': 0.5982269269604801, 'colsample_bytree': 0.4261078028952574}. Best is trial 8 with value: 0.5750751628528479.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:41:37,510]\u001b[0m Trial 10 finished with value: 0.5037581426423918 and parameters: {'booster': 'gbtree', 'lambda': 0.00011326066571698097, 'alpha': 0.0014065131724357483, 'subsample': 0.821074962202608, 'colsample_bytree': 0.25740706286966275, 'max_depth': 9, 'min_child_weight': 2, 'eta': 1.56223131041542e-08, 'gamma': 0.7752258415933184, 'grow_policy': 'depthwise'}. Best is trial 8 with value: 0.5750751628528479.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:42:34,508]\u001b[0m Trial 11 finished with value: 0.5597711708702188 and parameters: {'booster': 'gbtree', 'lambda': 7.864885848738156e-06, 'alpha': 0.012528741720909717, 'subsample': 0.3002708067525583, 'colsample_bytree': 0.7553280063152679, 'max_depth': 9, 'min_child_weight': 2, 'eta': 2.9210402905397018e-06, 'gamma': 1.6688570304605624e-08, 'grow_policy': 'lossguide'}. Best is trial 8 with value: 0.5750751628528479.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:43:47,645]\u001b[0m Trial 12 finished with value: 0.5580674795390012 and parameters: {'booster': 'gbtree', 'lambda': 1.6872394686691574e-05, 'alpha': 0.0005128981216839644, 'subsample': 0.4968563940774147, 'colsample_bytree': 0.858810148688333, 'max_depth': 9, 'min_child_weight': 2, 'eta': 3.368047580534136e-07, 'gamma': 2.1352588703997442e-08, 'grow_policy': 'lossguide'}. Best is trial 8 with value: 0.5750751628528479.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:44:17,625]\u001b[0m Trial 13 finished with value: 0.5866502421914147 and parameters: {'booster': 'gbtree', 'lambda': 0.9249883365854602, 'alpha': 0.02895065588592554, 'subsample': 0.2006606239955373, 'colsample_bytree': 0.5245365270408026, 'max_depth': 7, 'min_child_weight': 4, 'eta': 0.7410430062564227, 'gamma': 0.07103597166148733, 'grow_policy': 'lossguide'}. Best is trial 13 with value: 0.5866502421914147.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:44:58,909]\u001b[0m Trial 14 finished with value: 0.6242525471855688 and parameters: {'booster': 'gbtree', 'lambda': 0.9246557379114444, 'alpha': 0.0677132520300839, 'subsample': 0.814072756976796, 'colsample_bytree': 0.5476396729915024, 'max_depth': 7, 'min_child_weight': 4, 'eta': 0.9858626530586154, 'gamma': 0.2092629793493278, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.6242525471855688.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:45:40,340]\u001b[0m Trial 15 finished with value: 0.6204651745448472 and parameters: {'booster': 'gbtree', 'lambda': 0.9917432155536821, 'alpha': 0.08465451386994882, 'subsample': 0.721638724159748, 'colsample_bytree': 0.5534032338397372, 'max_depth': 7, 'min_child_weight': 5, 'eta': 0.9412136212961988, 'gamma': 0.007315603237969347, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.6242525471855688.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:46:10,559]\u001b[0m Trial 16 finished with value: 0.5947260731585101 and parameters: {'booster': 'gbtree', 'lambda': 0.9861950449674172, 'alpha': 0.4878912907188443, 'subsample': 0.6980163576850257, 'colsample_bytree': 0.5852195952551146, 'max_depth': 5, 'min_child_weight': 5, 'eta': 0.3433844529515909, 'gamma': 0.0020231056665631713, 'grow_policy': 'depthwise'}. Best is trial 14 with value: 0.6242525471855688.\u001b[0m\n",
            "\u001b[32m[I 2021-09-05 13:46:55,683]\u001b[0m Trial 17 finished with value: 0.5680808418239519 and parameters: {'booster': 'gbtree', 'lambda': 0.06846786309038778, 'alpha': 4.7644792920331693e-07, 'subsample': 0.7670609668519954, 'colsample_bytree': 0.6244711822938797, 'max_depth': 7, 'min_child_weight': 4, 'eta': 0.024160276596988516, 'gamma': 0.0015327053066398135, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.6242525471855688.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials:  18\n",
            "Best trial:\n",
            "  Value: 0.6242525471855688\n",
            "  Params: \n",
            "    booster: gbtree\n",
            "    lambda: 0.9246557379114444\n",
            "    alpha: 0.0677132520300839\n",
            "    subsample: 0.814072756976796\n",
            "    colsample_bytree: 0.5476396729915024\n",
            "    max_depth: 7\n",
            "    min_child_weight: 4\n",
            "    eta: 0.9858626530586154\n",
            "    gamma: 0.2092629793493278\n",
            "    grow_policy: lossguide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtRrsjoeKBS0",
        "outputId": "6c14bbc2-58dd-48e7-9e22-9748bc39abaf"
      },
      "source": [
        "folds = KFold(n_splits=10, shuffle=True, random_state=None)\n",
        "oof = np.zeros(len(train_df))\n",
        "predictions_xgb = np.zeros(len(test_df))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
        "    fold_ += 1\n",
        "    print(\"Fold {}\".format(fold_))\n",
        "    trn_data = xgb.DMatrix(X[trn_idx], label=y[trn_idx])\n",
        "    val_data = xgb.DMatrix(X[val_idx], label=y[val_idx])\n",
        "\n",
        "    num_round = 200\n",
        "    clf = xgb.train(param,trn_data, num_round,evals=[(val_data,'val')],verbose_eval= 25,early_stopping_rounds= 10)\n",
        "    x_val = xgb.DMatrix(X[val_idx])\n",
        "    oof[val_idx] = clf.predict(x_val)\n",
        "    \n",
        "    test = xgb.DMatrix(X_test)\n",
        "    predictions_xgb += clf.predict(test) / folds.n_splits\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1\n",
            "[0]\tval-auc:0.534664\n",
            "Will train until val-auc hasn't improved in 10 rounds.\n",
            "[25]\tval-auc:0.713399\n",
            "[50]\tval-auc:0.725747\n",
            "Stopping. Best iteration:\n",
            "[40]\tval-auc:0.727797\n",
            "\n",
            "Fold 2\n",
            "[0]\tval-auc:0.53438\n",
            "Will train until val-auc hasn't improved in 10 rounds.\n",
            "[25]\tval-auc:0.719695\n",
            "Stopping. Best iteration:\n",
            "[35]\tval-auc:0.727052\n",
            "\n",
            "Fold 3\n",
            "[0]\tval-auc:0.538202\n",
            "Will train until val-auc hasn't improved in 10 rounds.\n",
            "[25]\tval-auc:0.718837\n",
            "[50]\tval-auc:0.728379\n",
            "Stopping. Best iteration:\n",
            "[43]\tval-auc:0.7291\n",
            "\n",
            "Fold 4\n",
            "[0]\tval-auc:0.531355\n",
            "Will train until val-auc hasn't improved in 10 rounds.\n",
            "[25]\tval-auc:0.717264\n",
            "[50]\tval-auc:0.726361\n",
            "Stopping. Best iteration:\n",
            "[47]\tval-auc:0.72722\n",
            "\n",
            "Fold 5\n",
            "[0]\tval-auc:0.537237\n",
            "Will train until val-auc hasn't improved in 10 rounds.\n",
            "[25]\tval-auc:0.717196\n",
            "[50]\tval-auc:0.726292\n",
            "Stopping. Best iteration:\n",
            "[44]\tval-auc:0.72792\n",
            "\n",
            "Fold 6\n",
            "[0]\tval-auc:0.531013\n",
            "Will train until val-auc hasn't improved in 10 rounds.\n",
            "[25]\tval-auc:0.717763\n",
            "Stopping. Best iteration:\n",
            "[34]\tval-auc:0.726405\n",
            "\n",
            "Fold 7\n",
            "[0]\tval-auc:0.534389\n",
            "Will train until val-auc hasn't improved in 10 rounds.\n",
            "[25]\tval-auc:0.714461\n",
            "[50]\tval-auc:0.724586\n",
            "Stopping. Best iteration:\n",
            "[43]\tval-auc:0.727098\n",
            "\n",
            "Fold 8\n",
            "[0]\tval-auc:0.533971\n",
            "Will train until val-auc hasn't improved in 10 rounds.\n",
            "[25]\tval-auc:0.719632\n",
            "[50]\tval-auc:0.729063\n",
            "Stopping. Best iteration:\n",
            "[50]\tval-auc:0.729063\n",
            "\n",
            "Fold 9\n",
            "[0]\tval-auc:0.534837\n",
            "Will train until val-auc hasn't improved in 10 rounds.\n",
            "[25]\tval-auc:0.717981\n",
            "[50]\tval-auc:0.72616\n",
            "Stopping. Best iteration:\n",
            "[43]\tval-auc:0.728682\n",
            "\n",
            "Fold 10\n",
            "[0]\tval-auc:0.532848\n",
            "Will train until val-auc hasn't improved in 10 rounds.\n",
            "[25]\tval-auc:0.712803\n",
            "[50]\tval-auc:0.722947\n",
            "Stopping. Best iteration:\n",
            "[46]\tval-auc:0.723609\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qBJBP0geqQ_5",
        "outputId": "d0c52668-6b86-4471-e143-fbdbf4e48387"
      },
      "source": [
        "preds_ac = [int(i > .5) for i in oof]\n",
        "print(\"\\n\\n\")\n",
        "print('-'*1000)\n",
        "print(\"ROC CV score: {:<8.5f}\".format(roc_auc_score(train_df['claim'].values, oof)))\n",
        "print(\"Accuracy CV score: {:<8.5f}\".format(accuracy_score(train_df['claim'].values, preds_ac)))\n",
        "print('-'*1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ROC CV score: 0.72509 \n",
            "Accuracy CV score: 0.67853 \n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuDBeAQFL1e2"
      },
      "source": [
        "### Stacking Classificator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq4YAxoNL1VE"
      },
      "source": [
        "from mlxtend.classifier import StackingCVClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY0aKy5OvpjN"
      },
      "source": [
        "lgbm_params = {'bagging_fraction': 0.9556136342570964,\n",
        " 'bagging_freq': 7,\n",
        " 'feature_fraction': 0.9093664719465578,\n",
        " 'lambda_l1': 0.0009731112192839761,\n",
        " 'lambda_l2': 0.05022674259693464,\n",
        " 'metric': 'auc',\n",
        " 'min_child_samples': 53,\n",
        " 'num_leaves': 136,\n",
        " 'objective': 'binary' }\n",
        "\n",
        "xgb_params = {'alpha': 0.9286263881717246,\n",
        "  'eval_metric' : 'auc',\n",
        " 'colsample_bytree': 0.7317911276911447,\n",
        " 'eta': 0.4994795364431274,\n",
        " 'gamma': 1.7554366442993805e-08,\n",
        " 'grow_policy': 'lossguide',\n",
        " 'lambda': 1.0681330380503332e-08,\n",
        " 'max_depth': 9,\n",
        " 'min_child_weight': 5,\n",
        " 'subsample': 0.9961101392862352}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3nXSEJUL1Sz"
      },
      "source": [
        "clf1 = RandomForestClassifier()\n",
        "clf2 = lgb.LGBMClassifier(**lgbm_params)\n",
        "clf3 = xgb.XGBRFClassifier(**xgb_params)\n",
        "lr = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dgkVkIkL1Qw"
      },
      "source": [
        "sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],\n",
        "                            meta_classifier=lr,cv = 5,\n",
        "                            use_probas = True,use_features_in_secondary = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuniUpCCyDCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adcf0eb6-4004-487d-cbe6-e44667cfd4d5"
      },
      "source": [
        "sclf.fit(X,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingCVClassifier(classifiers=[RandomForestClassifier(bootstrap=True,\n",
              "                                                         ccp_alpha=0.0,\n",
              "                                                         class_weight=None,\n",
              "                                                         criterion='gini',\n",
              "                                                         max_depth=None,\n",
              "                                                         max_features='auto',\n",
              "                                                         max_leaf_nodes=None,\n",
              "                                                         max_samples=None,\n",
              "                                                         min_impurity_decrease=0.0,\n",
              "                                                         min_impurity_split=None,\n",
              "                                                         min_samples_leaf=1,\n",
              "                                                         min_samples_split=2,\n",
              "                                                         min_weight_fraction_leaf=0.0,\n",
              "                                                         n_estimators=100,\n",
              "                                                         n_jobs=None,\n",
              "                                                         oob_score=...\n",
              "                     meta_classifier=LogisticRegression(C=1.0,\n",
              "                                                        class_weight=None,\n",
              "                                                        dual=False,\n",
              "                                                        fit_intercept=True,\n",
              "                                                        intercept_scaling=1,\n",
              "                                                        l1_ratio=None,\n",
              "                                                        max_iter=100,\n",
              "                                                        multi_class='auto',\n",
              "                                                        n_jobs=None,\n",
              "                                                        penalty='l2',\n",
              "                                                        random_state=None,\n",
              "                                                        solver='lbfgs',\n",
              "                                                        tol=0.0001, verbose=0,\n",
              "                                                        warm_start=False),\n",
              "                     shuffle=True, store_train_meta_features=False,\n",
              "                     stratify=True, use_clones=True,\n",
              "                     use_features_in_secondary=True, use_probas=True,\n",
              "                     verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzJyRmOxLmzZ"
      },
      "source": [
        "## Predctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VztbgYDxyPOB"
      },
      "source": [
        "predictions_stack = sclf.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vppYeY_mL1I_"
      },
      "source": [
        "total_pred = 0.5*np.expm1(predictions_lgbm) + 0.5*np.expm1(predictions_xgb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbpL4gVerSdU"
      },
      "source": [
        "sub_df = pd.DataFrame({\"id\":test_df[\"id\"].values})\n",
        "sub_df[\"claim\"] = predictions_stack[:,1]\n",
        "sub_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkopEJy3SlPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d382c4-0d89-4192-e4fd-505e0c0c8369"
      },
      "source": [
        "! kaggle competitions submit -c tabular-playground-series-sep-2021 -f submission.csv -m \"LightGMB XGBoost Stack\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "100% 12.8M/12.8M [00:00<00:00, 23.6MB/s]\n",
            "400 - Bad Request\n"
          ]
        }
      ]
    }
  ]
}